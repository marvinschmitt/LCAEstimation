{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../BayesFlow')))\n",
    "\n",
    "from bayesflow.trainers import MetaTrainer, ParameterEstimationTrainer\n",
    "from bayesflow.networks import SequenceNet, EvidentialNetwork, InvertibleNetwork, InvariantNetwork\n",
    "from bayesflow.amortizers import MetaAmortizer, SingleModelAmortizer\n",
    "from bayesflow.models import GenerativeModel\n",
    "from bayesflow.diagnostics import true_vs_estimated\n",
    "\n",
    "from generative_models import batch_generate_levy_lca, generate_levy_lca\n",
    "\n",
    "from meta_aux_classes import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext Cython\n",
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prior(batch_size, n_models, p_vals=None):\n",
    "    \"\"\"\n",
    "    Samples from the models' prior batch size times and converts to one-hot.\n",
    "    Assumes equal model priors.\n",
    "    ----------\n",
    "    \n",
    "    Arguments:\n",
    "    batch_size : int  -- the number of samples to draw from the prior\n",
    "    ----------\n",
    "    \n",
    "    Returns:\n",
    "    m_true : np.ndarray of shape (batch_size, theta_dim) -- the samples batch of parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Equal priors, if nothign specified\n",
    "    if p_vals is None:\n",
    "        p_vals = [1/n_models] * n_models\n",
    "    m_idx = np.random.choice(n_models, size=batch_size, p=p_vals).astype(np.int32)\n",
    "    return m_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCAPrior:\n",
    "    def __init__(self, n_conditions=2, n_accumulators=2):\n",
    "        self.n_conditions = n_conditions\n",
    "        self.n_accumulators = n_accumulators\n",
    "    \n",
    "    def __call__(self, batch_size):\n",
    "        p = np.random.uniform(0.5, 2.00, size=(batch_size, self.n_conditions * self.n_accumulators)).astype('double')\n",
    "        a = np.random.uniform(0.5, 1.5, size=batch_size).astype('double')        \n",
    "        beta = np.random.uniform(0.1, 0.9, size=batch_size).astype('double')\n",
    "        kappa = np.random.uniform(0.1, 0.9, size=batch_size).astype('double')\n",
    "        ndt = np.random.uniform(0.2, 0.5, size=batch_size).astype('double')\n",
    "        # alpha = np.random.uniform(1.0, 2.0, size=batch_size).astype('double')\n",
    "        \n",
    "        params = np.column_stack((p, a, beta, kappa, ndt))\n",
    "        return params\n",
    "    \n",
    "\n",
    "class LCASimulator:\n",
    "    def __init__(self, n_conditions=2, n_accumulators=2, s=0.1):\n",
    "        self.n_conditions = n_conditions\n",
    "        self.n_accumulators = n_accumulators\n",
    "        self.s = s  # noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def partition(n, c):\n",
    "        \"\"\"Partitions the integer n into c equally sized chunks and returns an c-lengthed array of chunk sizes.\"\"\"\n",
    "        return [n // c + (1 if x < n % c else 0)\n",
    "                      for x in range (c)]\n",
    "        \n",
    "    def __call__(self, params, n_obs):\n",
    "        n_sim = params.shape[0]\n",
    "        \n",
    "        # total number of drift rates = n_cond * n_acc\n",
    "        n_p = self.n_conditions * self.n_accumulators\n",
    "        \n",
    "        # Split n_obs equally between conditions \n",
    "        n_obs_cond = self.partition(n_obs, self.n_conditions)\n",
    "\n",
    "        # Shared parameters\n",
    "        a, beta, kappa, ndt = tuple(params.T[n_p:])\n",
    "        alpha = np.full_like(a, 2.0, dtype='double')   # Levy alpha-stable parameter\n",
    "        \n",
    "        # Individual parameters per cond and accumulator: p, x_acc\n",
    "        p_cond = params[:, :n_p].reshape((n_sim, self.n_accumulators, self.n_conditions))\n",
    "        x_acc = np.zeros_like(p_cond, dtype='double')\n",
    "        \n",
    "        # initialize sim_data array\n",
    "        sim_data = np.empty((n_sim, n_obs), dtype=np.float32)\n",
    "        \n",
    "        # indexing along n_obs dimension\n",
    "        start_idx = end_idx = 0\n",
    "        for c in range(self.n_conditions):\n",
    "            start_idx, end_idx = end_idx, end_idx + n_obs_cond[c]  # increment by current n_obs_cond\n",
    "            cond_data = batch_generate_levy_lca(n_obs_cond[c], \n",
    "                                                p_cond[:, :, c], x_acc[:, :, c],  # individual \n",
    "                                                kappa, beta, a, ndt, alpha, self.s)       # shared\n",
    "            sim_data[:, start_idx:end_idx] = np.array(cond_data, dtype=np.float32)\n",
    "        \n",
    "        cond_row = np.concatenate([n_obs_cond[c] * [c] for c in range(self.n_conditions)])\n",
    "        cond_arr = np.stack(n_sim * [cond_row])\n",
    "        sim_data = np.stack((sim_data, cond_arr), axis=-1)\n",
    "        \n",
    "        return sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-6db58a26167d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mneg_rt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrt\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mp_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_rt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 6)"
     ]
    }
   ],
   "source": [
    "prior = LCAPrior(n_accumulators=2, n_conditions=1)\n",
    "simulator = LCASimulator(n_accumulators=2, n_conditions=1, s=0.1)\n",
    "generative_model = GenerativeModel(prior=prior, simulator=simulator)\n",
    "\n",
    "theta, x = generative_model(1, 1000)\n",
    "rt, cat = x[0].T[0], x[0].T[1]\n",
    "rt = rt[cat==0]\n",
    "pos_rt = rt[rt>0]\n",
    "neg_rt = - rt[rt<0]\n",
    "\n",
    "p1, p2, a, beta, kappa, ndt = tuple(theta[0].T)\n",
    "p_pos, p_neg = np.max((p1, p2)), np.min((p1, p2))\n",
    "acc = pos_rt.shape[0] / rt.shape[0] * 100\n",
    "\n",
    "print(f\"Accuracy: {acc:.2f}%\")\n",
    "print(f\"Drift rates: {p_pos:.3f}, {p_neg:.3f}\")\n",
    "print(f\"Other params: a={a:.3f}, kappa={kappa:.3f}, beta={beta:.3f}, ndt={ndt:.3f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "sns.histplot(neg_rt, kde=True, bins=50, linewidth=0.1,\n",
    "             color='green', alpha=0.2)\n",
    "for p in ax.patches:  # turn the histogram upside down\n",
    "    p.set_height(-p.get_height())\n",
    "for l in ax.lines:  # turn the kde curve upside down\n",
    "    l.set_ydata(-l.get_ydata())\n",
    "\n",
    "sns.histplot(pos_rt, kde=True, bins=50, linewidth=0.1,\n",
    "             color='blue', alpha=0.2)\n",
    "\n",
    "ax.set_xticks(np.arange(0, 5, 1))\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "pos_ticks = np.array([t for t in ax.get_yticks() if t > 0])\n",
    "ticks = np.concatenate([-pos_ticks[::-1], [0], pos_ticks])\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_yticklabels([f'{abs(t):.2f}' for t in ticks])\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "\n",
    "plt.axvline(0, color=\"grey\", ls='--')\n",
    "plt.axvline(5, color=\"grey\", ls='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 8 # \n",
    "\n",
    "summary_meta = {\n",
    "    'n_dense_s1': 2,\n",
    "    'n_dense_s2': 2,\n",
    "    'n_dense_s3': 2,\n",
    "    'n_equiv':    2,\n",
    "    'dense_s1_args': {'activation': 'relu', 'units': 128},\n",
    "    'dense_s2_args': {'activation': 'relu', 'units': 128},\n",
    "    'dense_s3_args': {'activation': 'relu', 'units': 128},\n",
    "}\n",
    "\n",
    "summary_net = InvariantNetwork(summary_meta)  # Output size 128\n",
    "\n",
    "\n",
    "inference_meta = {\n",
    "    'n_coupling_layers': 4,\n",
    "    's_args': {\n",
    "        'units': [64, 64, 64],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    't_args': {\n",
    "        'units': [64, 64, 64],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    'n_params': D,\n",
    "    'alpha': 1.9,\n",
    "    'permute': True\n",
    "}\n",
    "\n",
    "inference_net = InvertibleNetwork({'n_params': D})\n",
    "\n",
    "amortizer = SingleModelAmortizer(inference_net, summary_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = LCAPrior(n_accumulators=2, n_conditions=2)\n",
    "simulator = LCASimulator(n_accumulators=2, n_conditions=2, s=0.1)\n",
    "generative_model = GenerativeModel(prior=prior, simulator=simulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ParameterEstimationTrainer(amortizer,\n",
    "                      generative_model,\n",
    "                      learning_rate=0.0001,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f26ca086074a3ab86319a744c6fa28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c24dc5cee344d6aad800505d0aa749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = trainer.train_online(epochs=2, iterations_per_epoch=100, batch_size=16,\n",
    "                                            n_obs=np.random.randint(100, 501))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = trainer.train_rounds(epochs=2, rounds=2, sim_per_round=100, batch_size=32, \n",
    "                              n_obs=np.random.randint(100, 501))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate (quick and dirty)\n",
    "p, x = trainer._forward_inference(100, 100)\n",
    "param_samples = trainer.network.sample(x, n_samples=1000)\n",
    "param_means = param_samples.mean(axis=0)\n",
    "true_vs_estimated(p, param_means, ['p11', 'p12', 'p21', 'p22', 'a', 'beta', 'kappa', 'ndt'], figsize=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
